\subsection{PolyDice Loss}
医用画像セグメンテーションで広く使用されるDice Lossは，クラス不均衡に頑健であるが，全画像に対して固定的な形状を持つという制約がある．
本研究では，Dice Lossを多項式展開により拡張したPolyDice Loss\cite{polydice}，特にその実用的な形式であるPolyDice-1 Lossを採用する．
PolyDice-1 Lossは，単一のパラメータ$\epsilon$で損失関数の形状を制御でき，画像の難易度に応じて勾配の急峻さを調整することが可能となる．

\subsubsection{Dice Lossの定義}
画像サイズを $H \times W$とし，ピクセル位置を $(i, j)$ で表す
$\left(i \in \{1, ..., H\}, j \in \{1, ..., W\}\right)$．
セグメンテーションタスクにおいて，モデルの予測確率マップを$\hat{\mathbf{Y}} = \{\hat{y}_{i,j}\}_{i,j} \in \mathbb R ^ {H \times W}$，
その画像に対する正解マスクを$\mathbf{Y} = \{{y}_{i,j}\}_{i,j} \in \mathbb R ^ {H \times W}$とすると，Dice Lossは次式で定義される．

\begin{equation}
    \mathcal{L}_{\text{Dice}}(\hat{\mathbf{Y}}, \mathbf{Y}) = 1 - \frac{2 \sum_{j=1}^{W} \sum_{i=1}^{H} \hat{y}_{i, j} y_{i, j}}{\sum_{j=1}^{W} \sum_{i=1}^{H}(\hat{y_{i, j}} ^ 2 + y_{i, j} ^ 2)}
\end{equation}

\subsubsection{幾何学的解釈と多項式展開}

予測確率マップ$\hat{\mathbf{Y}}$と正解マスク$\mathbf{Y}$をそれぞれ長さ$HW$のベクトル$\hat{\mathbf{y}}, \mathbf{y}$
として平坦化すると，Dice Lossは以下のように分解できる．

\begin{equation}
    \mathcal{L}_{\text{Dice}} = 1 - s \cos \theta
\end{equation}
ここで，$s = \frac{2 \langle \hat{\mathbf{y}}, {\mathbf{y}} \rangle}{\Vert \hat{\mathbf{y}} \Vert ^ 2 + \Vert {\mathbf{y}} \Vert ^ 2}$はスケール成分，
$\theta = \arccos{\frac{\langle \hat{\mathbf{y}}, {\mathbf{y}}\rangle}{\Vert \hat{\mathbf{y}} \Vert \Vert {\mathbf{y}}\Vert}}$は2つの
ベクトル間の角度を表す．この分解により，Dice Lossはスケール成分$s$と$\cos \theta$の積として理解できる．

方向成分$\cos \theta$に対してTaylor展開を適用することで，PolyDice Lossの多項式表現を導出する．
学習が進むにつれて予測$\hat{\mathbf{y}}$は正解$\mathbf{y}$に近づくため，両ベクトルのなす角$\theta$は$0$に近づく．
この性質を利用し，$\cos{\theta}$を$\theta = 0$まわりでテイラー展開すると以下のように近似できる．
\begin{equation}
    \cos{\theta} = 1 - \frac{\theta ^ 2}{2!} + \frac{\theta ^ 4}{4!} - \cdots
\end{equation}
これをDice Lossに代入し，整理するとPolyDiceの一般形が得られる：

\begin{align}
    \mathcal{L}_{\text{PolyDice}} &= 1 - s\left(1 - \frac{\theta ^ 2}{2!} + \frac{\theta ^ 4}{4!} - \cdots\right) \\
    &= (1 - s) + s \sum_{k = 1}^{\infty} \alpha_k \theta ^ {2k}
\end{align}
ここで， $\alpha_k=\frac{(-1)^{k-1}}{(2k)!}$は各Taylor項の符号係数である．

\subsubsection{PolyDice-1 Loss}

PolyLoss\cite{leng2022polyloss}は分類タスクにおいてCross-Entropy Lossを多項式展開し，
第1項のみを調整可能とすることで実用的な性能向上を達成した．PolyDice Loss\cite{polydice}はこのアプローチをDice Lossに適用したものであり，
本研究では第1項のみを調整する PolyDice-1 Loss を採用する．
\begin{equation}
    \mathcal{L}_{\text{PolyDice-1}} = (1 - s) + s \left(\frac{1}{2} + \epsilon\right) \theta^2
\end{equation}
ここで，$\epsilon \in \mathbb{R}$は損失関数の形状を制御するハイパーパラメータである．
図\ref{polydice}に，$\epsilon$に応じたPolyDice-1 Lossの形状変化を示す．
$\epsilon > 0$では予測誤差に対するペナルティが強化され，$\epsilon < 0$では緩和される．
この柔軟な形状制御が可能な特性は，本研究の適応的学習フレームワークにおいて重要な役割を果たす．
後述する提案手法では，この$\epsilon$を動的に調整することで，個々の画像の難易度に応じた勾配制御を実現し，学習戦略をサンプル単位で最適化することを可能にしている．

\clearpage

\begin{figure}
    \includegraphics[width=\columnwidth]{figure/loss.pdf}
    \caption{Plot of PolyDice-1 Loss ($s = 0.1$)}
    \label{polydice}
\end{figure}

\clearpage

\subsection{MC Dropout}

\subsubsection{認識的不確実性と偶然的不確実性}

深層学習モデルの予測に伴う不確実性は，その発生源に基づいて偶然的不確実性と認識的不確実性に大別される\cite{smith2018understanding}．

偶然的不確実性（Aleatoric Uncertainty）は，撮像装置に起因するノイズや低解像度，
あるいは組織境界の物理的な曖昧さなど，データそのものに内在する情報不足に起因する．
この不確実性はデータの本質的な統計的性質であるため，同一ドメインの訓練データを追加しても解消されない特性を持つ．

認識的不確実性（Epistemic Uncertainty）は，モデルが未学習のパターンや，
訓練データの不足による知識の欠如に起因する．
これは適切な学習データを追加し、モデルが対象の分布をより詳細に記述することで低減が可能である．
認識的不確実性が高い画像は，モデルが安定した特徴表現を獲得できておらず，予測が不安定な状態にあることを示す．
本研究では，この認識的不確実性を画像のセグメンテーション難易度を反映する指標として活用する．

% モデルの予測に付随する不確実性を評価し，その依拠する要因を分離する指標として，相互情報量が広く用いられている．
% $T$ 回の推論結果を平均化した後の予測分布に対する不確実性であり，データとモデルの両方に由来する不確実性を示す予測エントロピーから，
% 個々の推論における不確実性の平均であり，
% データ固有のノイズや曖昧さに由来する偶然的不確実性を示す期待エントロピーを減ずることで，
% 以下の通り認識的不確実性を定量化できる．

% \begin{equation}
%     I_{i,j} = \underbrace{H\left( \frac{1}{T} \sum_{t=1}^{T} \hat{y}_{i,j}^{(t)} \right)}_{\text{Entropy of Mean}} - \underbrace{\frac{1}{T} \sum_{t=1}^{T} H\left( \hat{y}_{i,j}^{(t)} \right)}_{\text{Mean of Entropy}}
% \end{equation}

% ここで，$H(p)$ は2値分類におけるバイナリ・エントロピー関数であり，次式で定義される．

% \begin{equation}
%     H(p) = -p \log p - (1-p) \log (1-p)
% \end{equation}

\subsubsection{MC Dropout の原理と応用}

Dropoutは，ニューラルネットワークの過学習を防ぐ正則化手法として提案された\cite{JMLR:v15:srivastava14a}．
訓練時に各層のニューロンを確率$p$でランダムに不活性化することで，
モデルの汎化性能を向上させる．通常，推論時にはDropoutは無効化され，全ニューロンが活性化された状態で決定論的な予測が行われる．

MC Dropout\cite{pmlr-v48-gal16}は，学習時のみだけでなく，推論時にもDropoutを有効にすることで，モデルの認識的不確実性を推定する手法である．
Dropoutを有効にするした状態で推論を行うと，各推論において異なるニューロンが不活性化されるため，
実質的に異なる部分ネットワークによる予測が得られる．
同一入力に対してこの確率的推論を複数回実行することで，予測の分布を取得できる．

Gal and Ghahramani\cite{pmlr-v48-gal16}は，Dropout を適用したニューラルネットワークの学習が，
ベイズ推論における近似と数学的に等価であることを示した．この理論的枠組みにより，MC Dropout で得られる
予測分布は，モデルパラメータの事後分布に基づく予測不確実性，すなわち認識的不確実性の近似として解釈できる．

提案法では，MC Dropoutによる推定される認識的不確実性を，
画像のセグメンテーション難易度を反映する指標として活用し，損失関数の適応的制御に利用する．

